{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ste/.cache/huggingface/datasets/json/default-901973c7d83af16c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 1/1 [00:00<00:00, 380.13it/s]\n",
      "Loading cached processed dataset at /home/ste/.cache/huggingface/datasets/json/default-901973c7d83af16c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3d9795bc45b8494f.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "dataset = load_dataset('json',data_files='data.json').class_encode_column('artist')\n",
    "artists_mappings = dataset['train'].features['artist'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "def clean(example):\n",
    "    allowed_parts = ['verse','break','chorus','intro', 'interlude', 'bridge', 'outro']\n",
    "    for part in allowed_parts:\n",
    "        example['lyrics']=re.sub(\"\\[.*\"+part+\".*\\]\", f\"[{part}]\", example['lyrics'], flags=re.IGNORECASE)\n",
    "    example['lyrics']=re.sub(\"\\[(?!\"+\"|\".join(allowed_parts)+\").*?\\]\", \"\", example['lyrics'], flags=re.DOTALL)\n",
    "    example['lyrics']=re.sub(\"[0-9]+embed\", \"\", example['lyrics'], flags=re.IGNORECASE)\n",
    "    return example\n",
    "\n",
    "mapped_dataset = dataset.map(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "def list_song_parts(example):\n",
    "    parts = re.findall(r'\\[[^\\[\\]]+\\]',example['lyrics']) # Capture everything enclosed in square brackets\n",
    "    for i,part in enumerate(parts): \n",
    "        parts[i] = re.sub(r':.*(?=\\])','',part) # Remove everything from : to the closing bracket ] (Most lyrics contain the name of the singer of these parts e.g. [Chorus: 2 Chainz])\n",
    "    return {'parts': parts}\n",
    "parts = mapped_dataset['train'].map(list_song_parts,remove_columns=dataset['train'].column_names)\n",
    "\n",
    "parts:np.ndarray = np.unique([el for l in parts['parts'] for el in l ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[break] [bridge] [chorus] [interlude] [intro] [outro] [verse]\n"
     ]
    }
   ],
   "source": [
    "print(*parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XO TOUR Llif3 Lyrics[Intro]\n",
      "Are you alright?\n",
      "I'm alright, I'm quite alright\n",
      "And my money's right\n",
      "8… (Yeah)\n",
      "Countin' them bands\n",
      "All way to the top 'til they be fallin' over\n",
      "(Yeah, yeah, yeah)\n",
      "Countin' them bands\n",
      "On my way to the top 'til we fallin' over\n",
      "\n",
      "[Chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Shoulda saw the way she looked me in my eyes\n",
      "She said, \"Baby, I am not afraid to die\"\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "\n",
      "[Verse 1]\n",
      "Phantom that's all red, inside all white\n",
      "Like somethin' you ride a sled down, I just want that head\n",
      "My Brittany got mad, I'm barely her man now\n",
      "Everybody got the same swag now\n",
      "Watch the way that I tear it down\n",
      "Stackin' my bands all the way to the top\n",
      "All the way 'til my bands fallin' over\n",
      "Every time that you leave your spot\n",
      "Your girlfriend call me like, \"Come on over!\"\n",
      "I like the way that she treat me\n",
      "Gon' leave you, won't leave me, I call it that Casanova\n",
      "She say I'm insane, yeah\n",
      "I might blow my brain out (Hey)\n",
      "Xanny, help the pain, yeah\n",
      "Please, Xanny, make it go away\n",
      "I'm committed, not addicted, but it keep control of me\n",
      "All the pain, now I can't feel it\n",
      "I swear that it's slowin' me, yeah\n",
      "[Chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Saw the way she looked me in my eyes\n",
      "She said, \"I am not afraid to die\" (Yeah)\n",
      "All my friends are dead\n",
      "Push me to the edge (Yeah)\n",
      "All my friends are dead, yeah, ooh\n",
      "Push me to the edge\n",
      "All my friends are dead, yeah\n",
      "All my friends are dead, yeah\n",
      "\n",
      "[Verse 2]\n",
      "That is not your swag, I swear you fake hard\n",
      "Now these niggas wanna take my cadence\n",
      "Rain on 'em, thunderstorm, rain on 'em (Ooh, yeah)\n",
      "Medicine, lil' nigga, take some (Yeh, yeh)\n",
      "Fast car, NASCAR, race on 'em\n",
      "In the club, ain't got no ones, then we would beg them\n",
      "Clothes from overseas, got the racks and they all C-Notes\n",
      "You is not a G though\n",
      "Lookin' at you stackin' all your money, it all green though\n",
      "I was countin' that and these all twenties, that's a G-roll\n",
      "\n",
      "[Bridge]\n",
      "She say, \"You're the worst, you're the worst\"\n",
      "I cannot die because this my universe\n",
      "[Chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Shoulda saw the way she looked me in my eyes\n",
      "She said, \"Baby, I am not afraid to die\"\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge780Embed\n",
      "----------------\n",
      "XO TOUR Llif3 Lyrics[intro]\n",
      "Are you alright?\n",
      "I'm alright, I'm quite alright\n",
      "And my money's right\n",
      "8… (Yeah)\n",
      "Countin' them bands\n",
      "All way to the top 'til they be fallin' over\n",
      "(Yeah, yeah, yeah)\n",
      "Countin' them bands\n",
      "On my way to the top 'til we fallin' over\n",
      "\n",
      "[chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Shoulda saw the way she looked me in my eyes\n",
      "She said, \"Baby, I am not afraid to die\"\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "\n",
      "[verse]\n",
      "Phantom that's all red, inside all white\n",
      "Like somethin' you ride a sled down, I just want that head\n",
      "My Brittany got mad, I'm barely her man now\n",
      "Everybody got the same swag now\n",
      "Watch the way that I tear it down\n",
      "Stackin' my bands all the way to the top\n",
      "All the way 'til my bands fallin' over\n",
      "Every time that you leave your spot\n",
      "Your girlfriend call me like, \"Come on over!\"\n",
      "I like the way that she treat me\n",
      "Gon' leave you, won't leave me, I call it that Casanova\n",
      "She say I'm insane, yeah\n",
      "I might blow my brain out (Hey)\n",
      "Xanny, help the pain, yeah\n",
      "Please, Xanny, make it go away\n",
      "I'm committed, not addicted, but it keep control of me\n",
      "All the pain, now I can't feel it\n",
      "I swear that it's slowin' me, yeah\n",
      "[chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Saw the way she looked me in my eyes\n",
      "She said, \"I am not afraid to die\" (Yeah)\n",
      "All my friends are dead\n",
      "Push me to the edge (Yeah)\n",
      "All my friends are dead, yeah, ooh\n",
      "Push me to the edge\n",
      "All my friends are dead, yeah\n",
      "All my friends are dead, yeah\n",
      "\n",
      "[verse]\n",
      "That is not your swag, I swear you fake hard\n",
      "Now these niggas wanna take my cadence\n",
      "Rain on 'em, thunderstorm, rain on 'em (Ooh, yeah)\n",
      "Medicine, lil' nigga, take some (Yeh, yeh)\n",
      "Fast car, NASCAR, race on 'em\n",
      "In the club, ain't got no ones, then we would beg them\n",
      "Clothes from overseas, got the racks and they all C-Notes\n",
      "You is not a G though\n",
      "Lookin' at you stackin' all your money, it all green though\n",
      "I was countin' that and these all twenties, that's a G-roll\n",
      "\n",
      "[bridge]\n",
      "She say, \"You're the worst, you're the worst\"\n",
      "I cannot die because this my universe\n",
      "[chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Shoulda saw the way she looked me in my eyes\n",
      "She said, \"Baby, I am not afraid to die\"\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n"
     ]
    }
   ],
   "source": [
    "# print(mapped_dataset)\n",
    "print(dataset['train'][1000]['lyrics'])\n",
    "print(\"----------------\")\n",
    "print(mapped_dataset['train'][1000]['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_parts = ['verse','break','chorus','intro', 'interlude', 'bridge', 'outro']\n",
    "def contains_one_of(allowed_parts,part:str):\n",
    "    for p in allowed_parts:\n",
    "        if p in part.lower(): return True\n",
    "    return False\n",
    "filtered_parts = np.unique([part for part in parts if contains_one_of(allowed_parts,part)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[break]', '[bridge]', '[chorus]', '[intro]', '[verse]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(filtered_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15: 99,\n",
       " 26: 99,\n",
       " 61: 99,\n",
       " 62: 99,\n",
       " 31: 99,\n",
       " 29: 99,\n",
       " 39: 99,\n",
       " 0: 99,\n",
       " 42: 99,\n",
       " 60: 99,\n",
       " 38: 99,\n",
       " 12: 98,\n",
       " 51: 98,\n",
       " 28: 99,\n",
       " 33: 99,\n",
       " 30: 99,\n",
       " 57: 92,\n",
       " 45: 99,\n",
       " 3: 99,\n",
       " 44: 99,\n",
       " 1: 99,\n",
       " 68: 99,\n",
       " 49: 99,\n",
       " 63: 99,\n",
       " 59: 99,\n",
       " 21: 99,\n",
       " 14: 99,\n",
       " 13: 99,\n",
       " 32: 99,\n",
       " 5: 99,\n",
       " 56: 99,\n",
       " 19: 97,\n",
       " 37: 99,\n",
       " 2: 99,\n",
       " 52: 99,\n",
       " 46: 99,\n",
       " 8: 99,\n",
       " 67: 99,\n",
       " 41: 99,\n",
       " 27: 99,\n",
       " 4: 99,\n",
       " 22: 99,\n",
       " 47: 99,\n",
       " 34: 96,\n",
       " 54: 99,\n",
       " 10: 99,\n",
       " 6: 99,\n",
       " 24: 99,\n",
       " 48: 99,\n",
       " 69: 99,\n",
       " 70: 99,\n",
       " 25: 99,\n",
       " 11: 99,\n",
       " 36: 99,\n",
       " 16: 99,\n",
       " 9: 99,\n",
       " 35: 99,\n",
       " 58: 99,\n",
       " 43: 99,\n",
       " 53: 99,\n",
       " 23: 99,\n",
       " 50: 98,\n",
       " 20: 99,\n",
       " 71: 99,\n",
       " 64: 99,\n",
       " 40: 99,\n",
       " 7: 99,\n",
       " 65: 99,\n",
       " 18: 99,\n",
       " 66: 99,\n",
       " 55: 99,\n",
       " 17: 99}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = {}\n",
    "def plot_dist(dataset):\n",
    "    for example in dataset:\n",
    "        if example['artist'] not in counts.keys():\n",
    "            counts[example['artist']] = 0\n",
    "        else:\n",
    "            counts[example['artist']] += 1\n",
    "    return counts\n",
    "plot_dist(mapped_dataset['train'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# This was made by ChatGPT, keep an eye for possible bugs\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel,AutoConfig\n",
    "\n",
    "# Define your dataset and data loader\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(val[index]) for key, val in self.data.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "\n",
    "# Define your triplet loss function\n",
    "class TripletLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        dist_pos = torch.norm(anchor - positive, p=2, dim=1)\n",
    "        dist_neg = torch.norm(anchor - negative, p=2, dim=1)\n",
    "        loss = torch.mean(torch.relu(dist_pos - dist_neg + self.margin))\n",
    "        return loss\n",
    "    \n",
    "# Define your model\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.dense_layer = torch.nn.Linear(self.transformer.config.hidden_size, self.transformer.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.transformer(input_ids, attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        embeddings = self.dense_layer(pooled_output)\n",
    "        return embeddings\n",
    "\n",
    "# Define your hyperparameters\n",
    "learning_rate = 2e-5\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Load your pre-trained model and define your model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TransformerModel(model_name)\n",
    "\n",
    "# # Freeze all the parameters in the pre-trained model\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define your training data\n",
    "data = {'input_ids': [[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1]]}\n",
    "train_dataset = SimpleDataset(data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n",
      "Epoch [1/3], Loss: 3.9939\n",
      "torch.Size([3, 768])\n",
      "Epoch [2/3], Loss: 0.0000\n",
      "torch.Size([3, 768])\n",
      "Epoch [3/3], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Define your optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Train your model\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        # Extract the input ids and attention masks from the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        # Encode the inputs using the pre-trained model\n",
    "        embeddings = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        # Split the embeddings into anchor, positive, and negative examples\n",
    "        embeddings = embeddings.reshape(-1, 3, model.transformer.config.hidden_size)\n",
    "        anchor = embeddings[:, 0]\n",
    "        positive = embeddings[:, 1]\n",
    "        negative = embeddings[:, 2]\n",
    "\n",
    "        # Compute the triplet loss and update the parameters\n",
    "        loss_fn = TripletLoss()\n",
    "        loss = loss_fn(anchor, positive, negative)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Print the loss every epoch\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7486034c368b724a4f5b8fe7b1c697f01f7a86a113524f74e082ec2feaceabdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
