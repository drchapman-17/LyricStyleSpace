{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1a5eae679f892839\n",
      "Found cached dataset json (C:/Users/tomma/.cache/huggingface/datasets/json/default-1a5eae679f892839/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.89it/s]\n",
      "Loading cached processed dataset at C:\\Users\\tomma\\.cache\\huggingface\\datasets\\json\\default-1a5eae679f892839\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-425175bc9efee20d.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset('json',data_files='data.json').class_encode_column('artist')\n",
    "artists_mappings = dataset['train'].features['artist'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7185/7185 [00:01<00:00, 6935.33ex/s]\n"
     ]
    }
   ],
   "source": [
    "def clean(example):\n",
    "    allowed_parts = ['verse','break','chorus','intro', 'interlude', 'bridge', 'outro']\n",
    "    for part in allowed_parts:\n",
    "        example['lyrics']=re.sub(\"\\[.*\"+part+\".*\\]\", f\"[{part}]\", example['lyrics'], flags=re.IGNORECASE)\n",
    "    example['lyrics']=re.sub(\"\\[(?!\"+\"|\".join(allowed_parts)+\").*?\\]\", \"\", example['lyrics'], flags=re.DOTALL)\n",
    "    example['lyrics']=re.sub(\"[0-9]+embed\", \"\", example['lyrics'], flags=re.IGNORECASE)\n",
    "    return example\n",
    "\n",
    "mapped_dataset = dataset.map(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7185/7185 [00:00<00:00, 18477.96ex/s]\n"
     ]
    }
   ],
   "source": [
    "def list_song_parts(example):\n",
    "    parts = re.findall(r'\\[[^\\[\\]]+\\]',example['lyrics']) # Capture everything enclosed in square brackets\n",
    "    for i,part in enumerate(parts): \n",
    "        parts[i] = re.sub(r':.*(?=\\])','',part) # Remove everything from : to the closing bracket ] (Most lyrics contain the name of the singer of these parts e.g. [Chorus: 2 Chainz])\n",
    "    return {'parts': parts}\n",
    "parts = mapped_dataset['train'].map(list_song_parts,remove_columns=dataset['train'].column_names)\n",
    "\n",
    "parts:np.ndarray = np.unique([el for l in parts['parts'] for el in l ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[break] [bridge] [chorus] [interlude] [intro] [outro] [verse]\n"
     ]
    }
   ],
   "source": [
    "print(*parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XO TOUR Llif3 Lyrics[Intro]\n",
      "Are you alright?\n",
      "I'm alright, I'm quite alright\n",
      "And my money's right\n",
      "8… (Yeah)\n",
      "Countin' them bands\n",
      "All way to the top 'til they be fallin' over\n",
      "(Yeah, yeah, yeah)\n",
      "Countin' them bands\n",
      "On my way to the top 'til we fallin' over\n",
      "\n",
      "[Chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Shoulda saw the way she looked me in my eyes\n",
      "She said, \"Baby, I am not afraid to die\"\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "\n",
      "[Verse 1]\n",
      "Phantom that's all red, inside all white\n",
      "Like somethin' you ride a sled down, I just want that head\n",
      "My Brittany got mad, I'm barely her man now\n",
      "Everybody got the same swag now\n",
      "Watch the way that I tear it down\n",
      "Stackin' my bands all the way to the top\n",
      "All the way 'til my bands fallin' over\n",
      "Every time that you leave your spot\n",
      "Your girlfriend call me like, \"Come on over!\"\n",
      "I like the way that she treat me\n",
      "Gon' leave you, won't leave me, I call it that Casanova\n",
      "She say I'm insane, yeah\n",
      "I might blow my brain out (Hey)\n",
      "Xanny, help the pain, yeah\n",
      "Please, Xanny, make it go away\n",
      "I'm committed, not addicted, but it keep control of me\n",
      "All the pain, now I can't feel it\n",
      "I swear that it's slowin' me, yeah\n",
      "[Chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Saw the way she looked me in my eyes\n",
      "She said, \"I am not afraid to die\" (Yeah)\n",
      "All my friends are dead\n",
      "Push me to the edge (Yeah)\n",
      "All my friends are dead, yeah, ooh\n",
      "Push me to the edge\n",
      "All my friends are dead, yeah\n",
      "All my friends are dead, yeah\n",
      "\n",
      "[Verse 2]\n",
      "That is not your swag, I swear you fake hard\n",
      "Now these niggas wanna take my cadence\n",
      "Rain on 'em, thunderstorm, rain on 'em (Ooh, yeah)\n",
      "Medicine, lil' nigga, take some (Yeh, yeh)\n",
      "Fast car, NASCAR, race on 'em\n",
      "In the club, ain't got no ones, then we would beg them\n",
      "Clothes from overseas, got the racks and they all C-Notes\n",
      "You is not a G though\n",
      "Lookin' at you stackin' all your money, it all green though\n",
      "I was countin' that and these all twenties, that's a G-roll\n",
      "\n",
      "[Bridge]\n",
      "She say, \"You're the worst, you're the worst\"\n",
      "I cannot die because this my universe\n",
      "[Chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Shoulda saw the way she looked me in my eyes\n",
      "She said, \"Baby, I am not afraid to die\"\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge780Embed\n",
      "----------------\n",
      "XO TOUR Llif3 Lyrics[intro]\n",
      "Are you alright?\n",
      "I'm alright, I'm quite alright\n",
      "And my money's right\n",
      "8… (Yeah)\n",
      "Countin' them bands\n",
      "All way to the top 'til they be fallin' over\n",
      "(Yeah, yeah, yeah)\n",
      "Countin' them bands\n",
      "On my way to the top 'til we fallin' over\n",
      "\n",
      "[chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Shoulda saw the way she looked me in my eyes\n",
      "She said, \"Baby, I am not afraid to die\"\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "\n",
      "[verse]\n",
      "Phantom that's all red, inside all white\n",
      "Like somethin' you ride a sled down, I just want that head\n",
      "My Brittany got mad, I'm barely her man now\n",
      "Everybody got the same swag now\n",
      "Watch the way that I tear it down\n",
      "Stackin' my bands all the way to the top\n",
      "All the way 'til my bands fallin' over\n",
      "Every time that you leave your spot\n",
      "Your girlfriend call me like, \"Come on over!\"\n",
      "I like the way that she treat me\n",
      "Gon' leave you, won't leave me, I call it that Casanova\n",
      "She say I'm insane, yeah\n",
      "I might blow my brain out (Hey)\n",
      "Xanny, help the pain, yeah\n",
      "Please, Xanny, make it go away\n",
      "I'm committed, not addicted, but it keep control of me\n",
      "All the pain, now I can't feel it\n",
      "I swear that it's slowin' me, yeah\n",
      "[chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Saw the way she looked me in my eyes\n",
      "She said, \"I am not afraid to die\" (Yeah)\n",
      "All my friends are dead\n",
      "Push me to the edge (Yeah)\n",
      "All my friends are dead, yeah, ooh\n",
      "Push me to the edge\n",
      "All my friends are dead, yeah\n",
      "All my friends are dead, yeah\n",
      "\n",
      "[verse]\n",
      "That is not your swag, I swear you fake hard\n",
      "Now these niggas wanna take my cadence\n",
      "Rain on 'em, thunderstorm, rain on 'em (Ooh, yeah)\n",
      "Medicine, lil' nigga, take some (Yeh, yeh)\n",
      "Fast car, NASCAR, race on 'em\n",
      "In the club, ain't got no ones, then we would beg them\n",
      "Clothes from overseas, got the racks and they all C-Notes\n",
      "You is not a G though\n",
      "Lookin' at you stackin' all your money, it all green though\n",
      "I was countin' that and these all twenties, that's a G-roll\n",
      "\n",
      "[bridge]\n",
      "She say, \"You're the worst, you're the worst\"\n",
      "I cannot die because this my universe\n",
      "[chorus]\n",
      "I don't really care if you cry\n",
      "On the real, you shoulda never lied\n",
      "Shoulda saw the way she looked me in my eyes\n",
      "She said, \"Baby, I am not afraid to die\"\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n",
      "All my friends are dead\n",
      "Push me to the edge\n"
     ]
    }
   ],
   "source": [
    "# print(mapped_dataset)\n",
    "print(dataset['train'][1000]['lyrics'])\n",
    "print(\"----------------\")\n",
    "print(mapped_dataset['train'][1000]['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_parts = ['verse','break','chorus','intro', 'interlude', 'bridge', 'outro']\n",
    "def contains_one_of(allowed_parts,part:str):\n",
    "    for p in allowed_parts:\n",
    "        if p in part.lower(): return True\n",
    "    return False\n",
    "filtered_parts = np.unique([part for part in parts if contains_one_of(allowed_parts,part)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[break]',\n",
       " '[bridge]',\n",
       " '[chorus]',\n",
       " '[interlude]',\n",
       " '[intro]',\n",
       " '[outro]',\n",
       " '[verse]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(filtered_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(dataset):\n",
    "    counts = {}\n",
    "    for example in dataset:\n",
    "        if example['artist'] not in counts.keys():\n",
    "            counts[example['artist']] = 0\n",
    "        else:\n",
    "            counts[example['artist']] += 1\n",
    "    plt.bar(counts.keys(), counts.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['artist', 'lyrics'],\n",
       "        num_rows: 5029\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['artist', 'lyrics'],\n",
       "        num_rows: 1510\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['artist', 'lyrics'],\n",
       "        num_rows: 646\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "tts_mapped_dataset = mapped_dataset['train'].train_test_split(train_size=0.7)\n",
    "mapped_dataset_valid = tts_mapped_dataset['test'].train_test_split(train_size=0.3)\n",
    "\n",
    "train_test_val_dataset = DatasetDict({\n",
    "    'train': tts_mapped_dataset['train'],\n",
    "    'test':mapped_dataset_valid['test'],\n",
    "    'val': mapped_dataset_valid['train']\n",
    "    \n",
    "})\n",
    "\n",
    "train_test_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.title(\"Train-Test-Valudation barplot\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlabel(\"artist\")\n",
    "plot_dist(train_test_val_dataset['train'])\n",
    "plot_dist(train_test_val_dataset['test'])\n",
    "plot_dist(train_test_val_dataset['val'])\n",
    "plt.legend(['train', 'test', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Language Model and Tokenizer Declaration:\n",
    "\n",
    "modelckpt=\"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelckpt)\n",
    "model = AutoModel.from_pretrained(modelckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.39ba/s]\n",
      "Loading cached processed dataset at C:\\Users\\tomma\\.cache\\huggingface\\datasets\\json\\default-1a5eae679f892839\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-54484fee37c60955.arrow\n",
      "Loading cached processed dataset at C:\\Users\\tomma\\.cache\\huggingface\\datasets\\json\\default-1a5eae679f892839\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-ce23a64fd2874b35.arrow\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "def prepare_train_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['lyrics'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "        )\n",
    "    tokenized_examples['labels'] = examples['artist']\n",
    "    return tokenized_examples\n",
    "\n",
    "train_features = train_test_val_dataset['train'].map(prepare_train_features, batched=True, remove_columns=train_test_val_dataset[\"train\"].column_names).with_format('torch')\n",
    "test_features = train_test_val_dataset['test'].map(prepare_train_features, batched=True, remove_columns=train_test_val_dataset[\"test\"].column_names).with_format('torch')\n",
    "val_features = train_test_val_dataset['val'].map(prepare_train_features, batched=True, remove_columns=train_test_val_dataset[\"val\"].column_names).with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_metric_learning\n",
      "  Downloading pytorch_metric_learning-2.0.1-py3-none-any.whl (109 kB)\n",
      "     -------------------------------------- 109.3/109.3 KB 6.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\tomma\\documents\\uni\\magistrale\\pyvenvs\\ml_venv\\lib\\site-packages (from pytorch_metric_learning) (1.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tomma\\documents\\uni\\magistrale\\pyvenvs\\ml_venv\\lib\\site-packages (from pytorch_metric_learning) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tomma\\documents\\uni\\magistrale\\pyvenvs\\ml_venv\\lib\\site-packages (from pytorch_metric_learning) (1.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tomma\\documents\\uni\\magistrale\\pyvenvs\\ml_venv\\lib\\site-packages (from pytorch_metric_learning) (1.22.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tomma\\documents\\uni\\magistrale\\pyvenvs\\ml_venv\\lib\\site-packages (from torch>=1.6.0->pytorch_metric_learning) (4.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\tomma\\documents\\uni\\magistrale\\pyvenvs\\ml_venv\\lib\\site-packages (from scikit-learn->pytorch_metric_learning) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tomma\\documents\\uni\\magistrale\\pyvenvs\\ml_venv\\lib\\site-packages (from scikit-learn->pytorch_metric_learning) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tomma\\documents\\uni\\magistrale\\pyvenvs\\ml_venv\\lib\\site-packages (from scikit-learn->pytorch_metric_learning) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tomma\\documents\\uni\\magistrale\\pyvenvs\\ml_venv\\lib\\site-packages (from tqdm->pytorch_metric_learning) (0.4.4)\n",
      "Installing collected packages: pytorch_metric_learning\n",
      "Successfully installed pytorch_metric_learning-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install pytorch_metric_learning\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# # This was made by ChatGPT, keep an eye for possible bugs\n",
    "\n",
    "# import torch\n",
    "# import transformers\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import Dataset\n",
    "# from transformers import AdamW\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "# from transformers import AutoTokenizer, AutoModel,AutoConfig\n",
    "\n",
    "# # Define your dataset and data loader\n",
    "# class SimpleDataset(Dataset):\n",
    "#     def __init__(self, data, tokenizer):\n",
    "#         self.data = data\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         item = {key: torch.tensor(val[index]) for key, val in self.data.items()}\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data['input_ids'])\n",
    "\n",
    "# # Define your triplet loss function\n",
    "# class TripletLoss(torch.nn.Module):\n",
    "#     def __init__(self, margin=1.0):\n",
    "#         super(TripletLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "\n",
    "#     def forward(self, anchor, positive, negative):\n",
    "#         dist_pos = torch.norm(anchor - positive, p=2, dim=1)\n",
    "#         dist_neg = torch.norm(anchor - negative, p=2, dim=1)\n",
    "#         loss = torch.mean(torch.relu(dist_pos - dist_neg + self.margin))\n",
    "#         return loss\n",
    "    \n",
    "# # Define your model\n",
    "# class TransformerModel(torch.nn.Module):\n",
    "#     def __init__(self, model_name):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "#         self.transformer = AutoModel.from_pretrained(model_name)\n",
    "#         self.dense_layer = torch.nn.Linear(self.transformer.config.hidden_size, self.transformer.config.hidden_size)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.transformer(input_ids, attention_mask)\n",
    "#         pooled_output = outputs[1]\n",
    "#         embeddings = self.dense_layer(pooled_output)\n",
    "#         return embeddings\n",
    "\n",
    "\n",
    "\n",
    "# # Load your pre-trained model and define your model\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = TransformerModel(model_name)\n",
    "\n",
    "# # # Freeze all the parameters in the pre-trained model\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Set the model to training mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "\n",
    "model = model.to(device)\n",
    "# # Freeze all the parameters in the pre-trained model\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Define training hyperparameters\n",
    "learning_rate = 2e-5\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Define your training data\n",
    "train_loader = DataLoader(train_features,batch_size=8)\n",
    "\n",
    "# Define your optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    # Train your model\n",
    "    for batch_idx,batch in enumerate(train_loader):\n",
    "        # Extract the input ids and attention masks from the batch\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Encode the inputs using the pre-trained model\n",
    "        embeddings = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                \"Epoch {} Iteration {}:  Number of mined triplets = {}\".format(\n",
    "                    epoch, batch_idx, mining_func.num_triplets\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Print the loss every epoch\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\NLP\\LyricStyleSpace\\metric_learning.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomma/Documents/Uni/Magistrale/NLP/LyricStyleSpace/metric_learning.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# train for 1 epoch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomma/Documents/Uni/Magistrale/NLP/LyricStyleSpace/metric_learning.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tomma/Documents/Uni/Magistrale/NLP/LyricStyleSpace/metric_learning.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
      "\u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\NLP\\LyricStyleSpace\\metric_learning.ipynb Cell 19\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/Documents/Uni/Magistrale/NLP/LyricStyleSpace/metric_learning.ipynb#X31sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/Documents/Uni/Magistrale/NLP/LyricStyleSpace/metric_learning.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Encode the inputs using the pre-trained model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomma/Documents/Uni/Magistrale/NLP/LyricStyleSpace/metric_learning.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m embeddings \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/Documents/Uni/Magistrale/NLP/LyricStyleSpace/metric_learning.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m indices_tuple \u001b[39m=\u001b[39m mining_func(embeddings, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/Documents/Uni/Magistrale/NLP/LyricStyleSpace/metric_learning.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(embeddings, labels, indices_tuple)\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1021\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1012\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1014\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1015\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1016\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1020\u001b[0m )\n\u001b[1;32m-> 1021\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1022\u001b[0m     embedding_output,\n\u001b[0;32m   1023\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1024\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1025\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1026\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1027\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1028\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1029\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1030\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1031\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1032\u001b[0m )\n\u001b[0;32m   1033\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1034\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:496\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    485\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    486\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    494\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    495\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    497\u001b[0m         hidden_states,\n\u001b[0;32m    498\u001b[0m         attention_mask,\n\u001b[0;32m    499\u001b[0m         head_mask,\n\u001b[0;32m    500\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    501\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    502\u001b[0m     )\n\u001b[0;32m    503\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    505\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    417\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    418\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    424\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    425\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 426\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    427\u001b[0m         hidden_states,\n\u001b[0;32m    428\u001b[0m         attention_mask,\n\u001b[0;32m    429\u001b[0m         head_mask,\n\u001b[0;32m    430\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    431\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    432\u001b[0m         past_key_value,\n\u001b[0;32m    433\u001b[0m         output_attentions,\n\u001b[0;32m    434\u001b[0m     )\n\u001b[0;32m    435\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    436\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:307\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    305\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[0;32m    308\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    310\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tomma\\Documents\\Uni\\Magistrale\\pyvenvs\\ML_venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "margin = 0.1\n",
    "loss_func = losses.TripletMarginLoss(margin=margin) # triplet loss with margin 0.1\n",
    "mining_func = miners.TripletMarginMiner(margin=margin, type_of_triplets=\"semihard\") # use semihard mining\n",
    "\n",
    "num_epochs = 1 # train for 1 epoch\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7486034c368b724a4f5b8fe7b1c697f01f7a86a113524f74e082ec2feaceabdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
